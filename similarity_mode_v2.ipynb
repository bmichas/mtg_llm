{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 01:28:08.730533: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 01:28:09.259076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/weap/masters_degree/masters_degree_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional, RepeatVector, Attention, Concatenate, Conv1D, MaxPooling1D, Concatenate, UpSampling1D, MultiHeadAttention, LayerNormalization, Add, GRU, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"embedding-data/simple-wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The greatest example has been in his present job ( then , Minister for Foreign Affairs ) , where he has perforce concentrated on Anglo-Irish relations and , in particular Northern Ireland ( .\n",
      "The greatest example has been in his present job ( then , Minister for Foreign Affairs ) , where he has perforce concentrated on Anglo-Irish relations and , in particular the North ( i.e. , Northern Ireland ) .\n",
      "President Hillery refused to speak to any opposition party politicians , but when Charles Haughey , who was Leader of the Opposition , had rang the President 's Office he threatened to end the career of the army officer answered and refused on Hillery 's explicit orders to put the call through to the President .\n",
      "His reputation rose further when opposition leaders under parliamentary privilege alleged that Taoiseach Charles Haughey , who in January 1982 had been Leader of the Opposition , had not merely rung the President 's Office but threatened to end the career of the army officer who took the call and who , on Hillery 's explicit instructions , had refused to put through the call to the President .\n",
      "He thought about returning to medicine , perhaps moving with his wife , Maeve ( also a doctor ) to Africa .\n",
      "He considered returning to medicine , perhaps moving with his wife , Maeve ( also a doctor ) to Africa .\n",
      "Hillery 's most famous policy was to force EEC member states to give equal pay to women .\n",
      "As Social Affairs Commissioner Hillery 's most famous policy initiative was to force EEC member states to give equal pay to women .\n",
      "When President Cearbhall Ã `` DÃ laigh resigned , Hillery agreed to become the Fianna FÃ il candidate in the election .\n",
      "When a furious President Ã `` DÃ laigh resigned , a deeply reluctant Hillery agreed to become the Fianna FÃ il candidate for the presidency .\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset['train']\n",
    "sentences = [sentence for pair in train_data['set'] for sentence in pair]\n",
    "for i in range(10):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Tukatongue Thallid', 'Moriok Replica', 'Faerie Mechanist', ...,\n",
       "        'Mogg Squad', 'Roots of Wisdom', 'Raven Guild Master'],\n",
       "       dtype=object),\n",
       " array(['When Tukatongue Thallid dies, create a 1/1 green Saproling creature token.1.011Creature — Fungus',\n",
       "        '{1}{B}, Sacrifice Moriok Replica: You draw two cards and you lose 2 life.3.022Artifact Creature — Warrior',\n",
       "        'Flying\\nWhen Faerie Mechanist enters the battlefield, look at the top three cards of your library. You may reveal an artifact card from among them and put it into your hand. Put the rest on the bottom of your library in any order.4.022Artifact Creature — Faerie Artificer',\n",
       "        ...,\n",
       "        'Mogg Squad gets -1/-1 for each other creature on the battlefield.2.033Creature — Goblin',\n",
       "        \"Mill three cards, then return a land card or Elf card from your graveyard to your hand. If you can't, draw a card. (To mill a card, put the top card of your library into your graveyard.)2.0nannanSorcery\",\n",
       "        'Whenever Raven Guild Master deals combat damage to a player, that player exiles the top ten cards of their library.\\nMorph {2}{U}{U} (You may cast this card face down as a 2/2 creature for {3}. Turn it face up any time for its morph cost.)3.011Creature — Human Wizard Mutant'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'data_download/MyDataMTGv2.json'\n",
    "# filepath = 'data_download/clean_df.csv'\n",
    "\n",
    "# def load_dataset(path):\n",
    "#     df = pd.read_csv('data_download/clean_df.csv')\n",
    "#     df['text'] = df['text'].astype(str).fillna('text')\n",
    "#     df['manaCost'] = df['manaCost'].astype(str).fillna('manaCost')\n",
    "#     df['toughness'] = df['toughness'].astype(str).fillna('toughness')\n",
    "#     df['power'] = df['power'].astype(str).fillna('power')\n",
    "#     df['type'] = df['type'].astype(str).fillna('type')\n",
    "#     df['description'] = df['text'] + ' manaCost ' + df['manaCost'] + ' toughness ' + df['toughness'] + ' power ' + df['power'] + ' types ' + df['type']\n",
    "#     return df['card_name'].values, df['description'].values\n",
    "\n",
    "def load_dataset(path):\n",
    "    df = pd.read_json(path).T\n",
    "    df['text'] = df['text'].astype(str).fillna('text')\n",
    "    df['manaValue'] = df['manaValue'].astype(str).fillna('manaValue')\n",
    "    df['toughness'] = df['toughness'].astype(str).fillna('toughness')\n",
    "    df['power'] = df['power'].astype(str).fillna('power')\n",
    "    df['type'] = df['type'].astype(str).fillna('type')\n",
    "    df['description'] = df['text'] + '' + df['manaValue'] + '' + df['toughness'] + '' + df['power'] + '' + df['type']\n",
    "    df_filtered = df[['description']].reset_index()\n",
    "    df_filtered.rename(columns={'index': 'card_name'}, inplace=True)\n",
    "    return df_filtered['card_name'].values, df_filtered['description'].values\n",
    "\n",
    "card_names, card_descriptions = load_dataset(filepath)\n",
    "\n",
    "card_names, card_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 512\n",
    "max_len_description = 50 \n",
    "max_len_name = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and padding\n",
    "tokenizer_sentences = Tokenizer(char_level=True,\n",
    "                      lower=True,\n",
    "                      filters='!\"#$%&()*,.:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "                    #   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "tokenizer_card_descriptions= Tokenizer(char_level=True,\n",
    "                             lower=True,\n",
    "                             filters='!\"#$%&()*,.:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "                    #   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "tokenizer_sentences.fit_on_texts(sentences)\n",
    "sequences_sentences = tokenizer_sentences.texts_to_sequences(sentences)\n",
    "padded_sequences_sentences = pad_sequences(sequences_sentences, maxlen=max_len_description, padding='post')\n",
    "\n",
    "tokenizer_card_descriptions.fit_on_texts(card_descriptions)\n",
    "sequences_card_descriptions = tokenizer_card_descriptions.texts_to_sequences(card_descriptions)\n",
    "padded_sequences_card_descriptions = pad_sequences(sequences_card_descriptions, maxlen=max_len_description, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer_sentences.word_index) + 1  # Plus 1 for padding\n",
    "\n",
    "padded_sequences_sentences = np.array(padded_sequences_sentences)\n",
    "target_padded_sequences_sentences= np.expand_dims(padded_sequences_sentences, -1)\n",
    "\n",
    "padded_sequences_card_descriptions = np.array(padded_sequences_card_descriptions)\n",
    "target_padded_sequences_card_descriptions= np.expand_dims(padded_sequences_card_descriptions, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(tokenizer, path='./models/tokenizer.pkl'):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "def load_tokenizer(path='./models/tokenizer.pkl'):\n",
    "    with open(path, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    return tokenizer\n",
    "\n",
    "save_tokenizer(tokenizer_card_descriptions)\n",
    "tokenizer_card_descriptions = load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name, path='./models'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    model.save(os.path.join(path, f\"{name}.keras\"))\n",
    "\n",
    "def load_model(name, path='./models'):\n",
    "    return tf.keras.models.load_model(os.path.join(path, f\"{name}.keras\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(vocab_size, input_length, embedding_dim):\n",
    "    # Encoder\n",
    "    input_text = Input(shape=(input_length,))\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_text)\n",
    "    x = LSTM(embedding_dim, return_sequences=False)(x)\n",
    "    \n",
    "    # Dense layer as the \"bottleneck\" embedding (this is our sentence embedding)\n",
    "    encoded = Dense(embedding_dim, activation='relu')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Dense(embedding_dim, activation='relu')(encoded)\n",
    "    x = tf.keras.layers.RepeatVector(input_length)(x)\n",
    "    x = LSTM(embedding_dim, return_sequences=True)(x)\n",
    "    decoded = Dense(vocab_size, activation='softmax')(x)\n",
    "    \n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(inputs=input_text, outputs=decoded)\n",
    "    \n",
    "    # Encoder model (for extracting embeddings)\n",
    "    encoder = Model(inputs=input_text, outputs=encoded)\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "def create_lstm_bilstm(vocab_size, input_length, embedding_dim):\n",
    "    # Input for both encoders\n",
    "    input_text = Input(shape=(input_length,))\n",
    "    \n",
    "    # BiLSTM Encoder\n",
    "    x1 = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_text)\n",
    "    x1 = Bidirectional(LSTM(embedding_dim, return_sequences=False))(x1)\n",
    "    encoded1 = Dense(embedding_dim, activation='relu')(x1)\n",
    "\n",
    "    # LSTM Encoder\n",
    "    x2 = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_text)\n",
    "    x2 = LSTM(embedding_dim, return_sequences=False)(x2)\n",
    "    encoded2 = Dense(embedding_dim, activation='relu')(x2)\n",
    "\n",
    "    # Combine Encodings\n",
    "    combined_encoding = Concatenate()([encoded1, encoded2])\n",
    "\n",
    "    # Decoder\n",
    "    x = Dense(embedding_dim * 2, activation='relu')(combined_encoding)  # Adjust for doubled embedding dimension\n",
    "    x = RepeatVector(input_length)(x)\n",
    "    x = LSTM(embedding_dim * 2, return_sequences=True)(x)\n",
    "    decoded = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "    # Models\n",
    "    autoencoder = Model(inputs=input_text, outputs=decoded)\n",
    "    encoder = Model(inputs=input_text, outputs=combined_encoding)\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "def create_bilstm_autoencoder(vocab_size, input_length, embedding_dim):\n",
    "    # Encoder\n",
    "    input_text = Input(shape=(input_length,))\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_text)\n",
    "    x = Bidirectional(LSTM(embedding_dim, return_sequences=False))(x)\n",
    "    \n",
    "    # Bottleneck\n",
    "    encoded = Dense(embedding_dim, activation='relu')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Dense(embedding_dim, activation='relu')(encoded)\n",
    "    x = RepeatVector(input_length)(x)\n",
    "    x = Bidirectional(LSTM(embedding_dim, return_sequences=True))(x)\n",
    "    decoded = Dense(vocab_size, activation='softmax')(x)\n",
    "    \n",
    "    # Models\n",
    "    autoencoder = Model(inputs=input_text, outputs=decoded)\n",
    "    encoder = Model(inputs=input_text, outputs=encoded)\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "\n",
    "def create_bilstm_autoencoder_attention(vocab_size, input_length, embedding_dim):\n",
    "    # Encoder\n",
    "    input_text = Input(shape=(input_length,))\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_text)\n",
    "    \n",
    "    # Bidirectional LSTM for richer context encoding\n",
    "    x = Bidirectional(LSTM(embedding_dim, return_sequences=True))(x)\n",
    "    \n",
    "    # Adding a Dropout layer to prevent overfitting\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Attention layer to focus on important words for MTG cards\n",
    "    # We calculate attention on the output of the LSTM\n",
    "    attention = Attention()([x, x])\n",
    "    x = Concatenate()([x, attention])  # Concatenate original LSTM output with attention output\n",
    "    \n",
    "    # Final dense layer as bottleneck embedding (sentence embedding)\n",
    "    x = LSTM(embedding_dim, return_sequences=False)(x)  # Flatten output for dense layer\n",
    "    encoded = Dense(embedding_dim, activation='relu')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Dense(embedding_dim, activation='relu')(encoded)\n",
    "    x = RepeatVector(input_length)(x)\n",
    "    \n",
    "    # Second LSTM layer for decoding\n",
    "    x = Bidirectional(LSTM(embedding_dim, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)  # Dropout in decoder for robustness\n",
    "    \n",
    "    # Final output layer with softmax activation\n",
    "    decoded = Dense(vocab_size, activation='softmax')(x)\n",
    "    \n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(inputs=input_text, outputs=decoded)\n",
    "    \n",
    "    # Encoder model (for extracting embeddings)\n",
    "    encoder = Model(inputs=input_text, outputs=encoded)\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "\n",
    "def create_cnn_lstm_autoencoder(vocab_size, input_length, embedding_dim):\n",
    "    # Encoder\n",
    "    input_text = Input(shape=(input_length,))\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_text)\n",
    "    x = Conv1D(embedding_dim, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = LSTM(embedding_dim, return_sequences=False)(x)\n",
    "    \n",
    "    # Bottleneck\n",
    "    encoded = Dense(embedding_dim, activation='relu')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Dense(embedding_dim, activation='relu')(encoded)\n",
    "    x = RepeatVector(input_length // 2)(x)\n",
    "    x = LSTM(embedding_dim, return_sequences=True)(x)\n",
    "    x = UpSampling1D(size=2)(x)\n",
    "    decoded = Dense(vocab_size, activation='softmax')(x)\n",
    "    \n",
    "    # Models\n",
    "    autoencoder = Model(inputs=input_text, outputs=decoded)\n",
    "    encoder = Model(inputs=input_text, outputs=encoded)\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "\n",
    "def transformer_encoder_decoder(vocab_size, input_length, embedding_dim, num_heads=4, dropout_rate=0.1):\n",
    "    # Positional Embedding Layer\n",
    "    class PositionalEmbedding(Layer):\n",
    "        def __init__(self, vocab_size, embedding_dim, input_length):\n",
    "            super(PositionalEmbedding, self).__init__()\n",
    "            self.embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "            self.positional_encoding = self.add_weight(\n",
    "                shape=(input_length, embedding_dim), initializer=\"zeros\", trainable=True\n",
    "            )\n",
    "        \n",
    "        def call(self, inputs):\n",
    "            seq_length = tf.shape(inputs)[1]\n",
    "            return self.embedding(inputs) + self.positional_encoding[:seq_length]\n",
    "\n",
    "    # Encoder\n",
    "    input_text = Input(shape=(input_length,))\n",
    "    x = PositionalEmbedding(vocab_size, embedding_dim, input_length)(input_text)\n",
    "    for _ in range(2):  # Stacking lightweight attention layers\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(x, x)\n",
    "        x = LayerNormalization()(Add()([x, attn_output]))\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    x = GRU(embedding_dim, return_sequences=False)(x)\n",
    "    \n",
    "    # Bottleneck\n",
    "    encoded = Dense(embedding_dim // 2, activation='relu')(x)\n",
    "    encoded = Dense(embedding_dim, activation='relu')(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    x = Dense(embedding_dim, activation='relu')(encoded)\n",
    "    x = RepeatVector(input_length)(x)\n",
    "    for _ in range(2):  # Reuse GRU with shared layers\n",
    "        x = GRU(embedding_dim, return_sequences=True)(x)\n",
    "        x = LayerNormalization()(x)\n",
    "    decoded = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "    # Models\n",
    "    autoencoder = Model(inputs=input_text, outputs=decoded)\n",
    "    encoder = Model(inputs=input_text, outputs=encoded)\n",
    "\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss',    \n",
    "    patience=5,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = [\"lstm\", \"bilstm\", \"lstm_bilstm\", \"bilstm_attention\", \"cnn_lstm\", \"transformer\"]\n",
    "model_names = [\"lstm\", \"cnn_lstm\"]\n",
    "# model_names = [\"transformer\"]\n",
    "# model_functions = [create_lstm_model, create_bilstm_autoencoder, create_lstm_bilstm, create_bilstm_autoencoder_attention, create_cnn_lstm_autoencoder, transformer_encoder_decoder]\n",
    "model_functions = [create_lstm_model, create_cnn_lstm_autoencoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: lstm\n",
      "Epoch 1/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m853s\u001b[0m 533ms/step - loss: 2.9001\n",
      "Epoch 2/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m856s\u001b[0m 536ms/step - loss: 2.3043\n",
      "Epoch 3/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m859s\u001b[0m 537ms/step - loss: 1.8891\n",
      "Epoch 4/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m860s\u001b[0m 538ms/step - loss: 1.5595\n",
      "Epoch 5/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 539ms/step - loss: 1.2918\n",
      "Epoch 6/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m862s\u001b[0m 539ms/step - loss: 1.0825\n",
      "Epoch 7/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m862s\u001b[0m 539ms/step - loss: 0.9474\n",
      "Epoch 8/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 540ms/step - loss: 0.8692\n",
      "Epoch 9/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 540ms/step - loss: 0.8031\n",
      "Epoch 10/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 540ms/step - loss: 0.6884\n",
      "Epoch 11/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 540ms/step - loss: 0.6499\n",
      "Epoch 12/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 540ms/step - loss: 0.6892\n",
      "Epoch 13/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m862s\u001b[0m 540ms/step - loss: 0.5551\n",
      "Epoch 14/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 540ms/step - loss: 0.5584\n",
      "Epoch 15/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 541ms/step - loss: 0.4229\n",
      "Epoch 16/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 541ms/step - loss: 0.4031\n",
      "Epoch 17/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.4159\n",
      "Epoch 18/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.5054\n",
      "Epoch 19/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m866s\u001b[0m 542ms/step - loss: 0.4191\n",
      "Epoch 20/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.3617\n",
      "Epoch 21/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.2841\n",
      "Epoch 22/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 541ms/step - loss: 0.6015\n",
      "Epoch 23/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 540ms/step - loss: 0.3348\n",
      "Epoch 24/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 541ms/step - loss: 0.2331\n",
      "Epoch 25/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 541ms/step - loss: 0.3131\n",
      "Epoch 26/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 541ms/step - loss: 0.2020\n",
      "Epoch 27/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 541ms/step - loss: 0.5603\n",
      "Epoch 28/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.2263\n",
      "Epoch 29/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.2182\n",
      "Epoch 30/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.2939\n",
      "Epoch 31/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m866s\u001b[0m 542ms/step - loss: 0.3356\n",
      "Epoch 32/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.1852\n",
      "Epoch 33/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 540ms/step - loss: 0.1478\n",
      "Epoch 34/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 541ms/step - loss: 0.1777\n",
      "Epoch 35/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.1538\n",
      "Epoch 36/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.1285\n",
      "Epoch 37/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.6215\n",
      "Epoch 38/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 541ms/step - loss: 0.2370\n",
      "Epoch 39/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.1562\n",
      "Epoch 40/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.1997\n",
      "Epoch 41/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m866s\u001b[0m 542ms/step - loss: 0.1321\n",
      "Epoch 42/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 541ms/step - loss: 0.1280\n",
      "Epoch 43/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m866s\u001b[0m 542ms/step - loss: 0.1573\n",
      "Epoch 44/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m895s\u001b[0m 560ms/step - loss: 0.1795\n",
      "Epoch 45/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m907s\u001b[0m 568ms/step - loss: 0.1602\n",
      "Epoch 46/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m881s\u001b[0m 551ms/step - loss: 0.1497\n",
      "Epoch 47/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m916s\u001b[0m 573ms/step - loss: 0.1097\n",
      "Epoch 48/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m874s\u001b[0m 547ms/step - loss: 0.1088\n",
      "Epoch 49/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m872s\u001b[0m 546ms/step - loss: 0.1107\n",
      "Epoch 50/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m871s\u001b[0m 545ms/step - loss: 0.1553\n",
      "Epoch 51/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 546ms/step - loss: 0.1050\n",
      "Epoch 52/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 546ms/step - loss: 0.0897\n",
      "Epoch 53/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 546ms/step - loss: 0.0910\n",
      "Epoch 54/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 546ms/step - loss: 0.1267\n",
      "Epoch 55/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 546ms/step - loss: 0.1305\n",
      "Epoch 56/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 546ms/step - loss: 0.0583\n",
      "Epoch 57/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 546ms/step - loss: 0.0773\n",
      "Epoch 58/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m874s\u001b[0m 547ms/step - loss: 0.0832\n",
      "Epoch 59/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m874s\u001b[0m 547ms/step - loss: 0.0990\n",
      "Epoch 60/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m874s\u001b[0m 547ms/step - loss: 0.0500\n",
      "Training Time: 52077.51 seconds\n",
      "Fine tunning:\n",
      "Epoch 1/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 292ms/step - loss: 1.4229\n",
      "Epoch 2/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 292ms/step - loss: 0.2184\n",
      "Epoch 3/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 282ms/step - loss: 0.2434\n",
      "Epoch 4/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 280ms/step - loss: 0.0953\n",
      "Epoch 5/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 280ms/step - loss: 0.2185\n",
      "Fine tunning time: 1165.52 seconds\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Model: cnn_lstm\n",
      "Epoch 1/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 381ms/step - loss: 2.8715\n",
      "Epoch 2/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 380ms/step - loss: 2.1990\n",
      "Epoch 3/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m607s\u001b[0m 380ms/step - loss: 1.7753\n",
      "Epoch 4/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 380ms/step - loss: 1.4931\n",
      "Epoch 5/60\n",
      "\u001b[1m1598/1598\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 379ms/step - loss: 1.3229\n",
      "Training Time: 3036.65 seconds\n",
      "Fine tunning:\n",
      "Epoch 1/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 176ms/step - loss: 2.3650\n",
      "Epoch 2/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 176ms/step - loss: 1.7886\n",
      "Epoch 3/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 177ms/step - loss: 1.5866\n",
      "Epoch 4/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 176ms/step - loss: 1.5182\n",
      "Epoch 5/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 177ms/step - loss: 1.4277\n",
      "Fine tunning time: 720.39 seconds\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n"
     ]
    }
   ],
   "source": [
    "def compute_embeddings(descriptions):\n",
    "    sequences = tokenizer_card_descriptions.texts_to_sequences(descriptions)\n",
    "    padded_seqs = pad_sequences(sequences, maxlen=max_len_description, padding='post')\n",
    "    return encoder.predict(padded_seqs)\n",
    "\n",
    "def get_card_description(querry):\n",
    "    index = np.where(card_names == querry)[0]\n",
    "    if index.size > 0:\n",
    "        return card_descriptions[index][0]\n",
    "    \n",
    "    return querry\n",
    "\n",
    "def get_card_name(querry):\n",
    "    card_index = np.where(card_descriptions == querry)[0][0]\n",
    "    return card_names[card_index]\n",
    "\n",
    "def find_similar_cards(querry, card_descriptions, card_embeddings, top_n=3):\n",
    "    card_description = get_card_description(querry)\n",
    "    query_embedding = compute_embeddings([card_description])[0]\n",
    "    similarities = cosine_similarity([query_embedding], card_embeddings)[0]\n",
    "    similar_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    return [(card_descriptions[i], similarities[i]) for i in similar_indices]\n",
    "\n",
    "model_predictions = []\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    model_name = \"_\" + model_names[i]\n",
    "    autoencoder_name = model_names[i] + \"_autoencoder\"\n",
    "    encoder_name = model_names[i] + \"_encoder\"\n",
    "    model_function = model_functions[i]\n",
    "    autoencoder, encoder = model_function(vocab_size, max_len_description, embedding_dim)\n",
    "    # autoencoder = load_model(autoencoder_name)\n",
    "    # encoder = load_model(encoder_name)\n",
    "    # print(autoencoder.summary(), encoder.summary())\n",
    "    autoencoder.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    \n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + model_name\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    print(f\"Model: {model_names[i]}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    autoencoder.fit(padded_sequences_sentences, target_padded_sequences_sentences, epochs=60, batch_size=128, callbacks=[early_stopping, tensorboard_callback])\n",
    "    # autoencoder.fit(padded_sequences_sentences, target_padded_sequences_sentences, epochs=1, batch_size=128, callbacks=[early_stopping])\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "    print(\"Fine tunning:\")\n",
    "    start_time = time.time()\n",
    "    autoencoder.fit(padded_sequences_card_descriptions, target_padded_sequences_card_descriptions, epochs=20, batch_size=32, callbacks=[early_stopping, tensorboard_callback])\n",
    "    # autoencoder.fit(padded_sequences_card_descriptions, target_padded_sequences_card_descriptions, epochs=1, batch_size=32, callbacks=[early_stopping])\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Fine tunning time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    card_embeddings = compute_embeddings(card_descriptions)\n",
    "    query_descriptions = ['Sol Ring', 'Structural Assault', 'Crossbow Ambush', 'Mephitic Draught', 'Sangromancer']\n",
    "    for query_description in query_descriptions:\n",
    "        card_predictions = [model_names[i], query_description]\n",
    "        similar_cards = find_similar_cards(query_description, card_descriptions, card_embeddings, 10)\n",
    "        for desc, score in similar_cards:        \n",
    "            card_predictions.append(get_card_name(desc))\n",
    "\n",
    "        model_predictions.append(card_predictions)\n",
    "\n",
    "\n",
    "    save_model(autoencoder, autoencoder_name)\n",
    "    save_model(encoder, encoder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Card Name</th>\n",
       "      <th>Similar Card 1</th>\n",
       "      <th>Similar Card 2</th>\n",
       "      <th>Similar Card 3</th>\n",
       "      <th>Similar Card 4</th>\n",
       "      <th>Similar Card 5</th>\n",
       "      <th>Similar Card 6</th>\n",
       "      <th>Similar Card 7</th>\n",
       "      <th>Similar Card 8</th>\n",
       "      <th>Similar Card 9</th>\n",
       "      <th>Similar Card 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lstm</td>\n",
       "      <td>Sol Ring</td>\n",
       "      <td>Sol Ring</td>\n",
       "      <td>Ur-Golem's Eye</td>\n",
       "      <td>Ur-Golem's Eye</td>\n",
       "      <td>Thran Dynamo</td>\n",
       "      <td>Wastes</td>\n",
       "      <td>Riverglide Pathway // Lavaglide Pathway</td>\n",
       "      <td>Riverglide Pathway // Lavaglide Pathway</td>\n",
       "      <td>Barkchannel Pathway // Tidechannel Pathway</td>\n",
       "      <td>Barkchannel Pathway // Tidechannel Pathway</td>\n",
       "      <td>Great Furnace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lstm</td>\n",
       "      <td>Structural Assault</td>\n",
       "      <td>Structural Assault</td>\n",
       "      <td>Thrilling Encore</td>\n",
       "      <td>Urborg Justice</td>\n",
       "      <td>Fresh Meat</td>\n",
       "      <td>Faith's Reward</td>\n",
       "      <td>Second Sunrise</td>\n",
       "      <td>Let the Galaxy Burn</td>\n",
       "      <td>Fatal Push</td>\n",
       "      <td>Cradle to Grave</td>\n",
       "      <td>Force of Despair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lstm</td>\n",
       "      <td>Crossbow Ambush</td>\n",
       "      <td>Silk Net</td>\n",
       "      <td>Vines of the Recluse</td>\n",
       "      <td>Shape the Sands</td>\n",
       "      <td>Crossbow Ambush</td>\n",
       "      <td>Silk Net</td>\n",
       "      <td>Gloomwidow's Feast</td>\n",
       "      <td>Spidery Grasp</td>\n",
       "      <td>Aim High</td>\n",
       "      <td>Treetop Defense</td>\n",
       "      <td>Aerial Volley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lstm</td>\n",
       "      <td>Mephitic Draught</td>\n",
       "      <td>Metalspinner's Puzzleknot</td>\n",
       "      <td>Mephitic Draught</td>\n",
       "      <td>Infernal Idol</td>\n",
       "      <td>Skeletal Scrying</td>\n",
       "      <td>Cut of the Profits</td>\n",
       "      <td>Wooden Sphere</td>\n",
       "      <td>Tablet of Epityr</td>\n",
       "      <td>Urza's Chalice</td>\n",
       "      <td>Crystal Rod</td>\n",
       "      <td>Soul Net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lstm</td>\n",
       "      <td>Sangromancer</td>\n",
       "      <td>Sangromancer</td>\n",
       "      <td>Bloodrite Invoker</td>\n",
       "      <td>Kalastria Highborn</td>\n",
       "      <td>Sanctum Seeker</td>\n",
       "      <td>Herald of the Pantheon</td>\n",
       "      <td>Acolyte of Aclazotz</td>\n",
       "      <td>High Fae Negotiator</td>\n",
       "      <td>Judge of Currents</td>\n",
       "      <td>Inspiring Cleric</td>\n",
       "      <td>Centaur Healer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cnn_lstm</td>\n",
       "      <td>Sol Ring</td>\n",
       "      <td>Sol Ring</td>\n",
       "      <td>Ur-Golem's Eye</td>\n",
       "      <td>Ur-Golem's Eye</td>\n",
       "      <td>Thran Dynamo</td>\n",
       "      <td>Bloodstone Cameo</td>\n",
       "      <td>Seashell Cameo</td>\n",
       "      <td>Troll-Horn Cameo</td>\n",
       "      <td>Drake-Skull Cameo</td>\n",
       "      <td>Great Furnace</td>\n",
       "      <td>Wastes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cnn_lstm</td>\n",
       "      <td>Structural Assault</td>\n",
       "      <td>Structural Assault</td>\n",
       "      <td>Let the Galaxy Burn</td>\n",
       "      <td>Mordor on the March</td>\n",
       "      <td>Fiery Encore</td>\n",
       "      <td>All of History, All at Once</td>\n",
       "      <td>Empty the Warrens</td>\n",
       "      <td>Urban Evolution</td>\n",
       "      <td>Escape to the Wilds</td>\n",
       "      <td>Flesh Allergy</td>\n",
       "      <td>Galvanic Relay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cnn_lstm</td>\n",
       "      <td>Crossbow Ambush</td>\n",
       "      <td>Crossbow Ambush</td>\n",
       "      <td>Shape the Sands</td>\n",
       "      <td>Vines of the Recluse</td>\n",
       "      <td>Silk Net</td>\n",
       "      <td>Silk Net</td>\n",
       "      <td>Gloomwidow's Feast</td>\n",
       "      <td>Treetop Defense</td>\n",
       "      <td>Aim High</td>\n",
       "      <td>Spidery Grasp</td>\n",
       "      <td>Angelic Ascension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cnn_lstm</td>\n",
       "      <td>Mephitic Draught</td>\n",
       "      <td>Metalspinner's Puzzleknot</td>\n",
       "      <td>Mephitic Draught</td>\n",
       "      <td>Infernal Idol</td>\n",
       "      <td>Profane Memento</td>\n",
       "      <td>Tithing Blade // Consuming Sepulcher</td>\n",
       "      <td>Thopter Foundry</td>\n",
       "      <td>Druidic Satchel</td>\n",
       "      <td>Ivory Crane Netsuke</td>\n",
       "      <td>Guild Globe</td>\n",
       "      <td>Sphere of the Suns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cnn_lstm</td>\n",
       "      <td>Sangromancer</td>\n",
       "      <td>Sangromancer</td>\n",
       "      <td>Bloodrite Invoker</td>\n",
       "      <td>Kalastria Highborn</td>\n",
       "      <td>Shattered Angel</td>\n",
       "      <td>Herald of the Pantheon</td>\n",
       "      <td>Deathgreeter</td>\n",
       "      <td>Bog-Strider Ash</td>\n",
       "      <td>Guardian of Cloverdell</td>\n",
       "      <td>Blood Seeker</td>\n",
       "      <td>Bleak Coven Vampires</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Name           Card Name             Similar Card 1  \\\n",
       "0       lstm            Sol Ring                   Sol Ring   \n",
       "1       lstm  Structural Assault         Structural Assault   \n",
       "2       lstm     Crossbow Ambush                   Silk Net   \n",
       "3       lstm    Mephitic Draught  Metalspinner's Puzzleknot   \n",
       "4       lstm        Sangromancer               Sangromancer   \n",
       "5   cnn_lstm            Sol Ring                   Sol Ring   \n",
       "6   cnn_lstm  Structural Assault         Structural Assault   \n",
       "7   cnn_lstm     Crossbow Ambush            Crossbow Ambush   \n",
       "8   cnn_lstm    Mephitic Draught  Metalspinner's Puzzleknot   \n",
       "9   cnn_lstm        Sangromancer               Sangromancer   \n",
       "\n",
       "         Similar Card 2        Similar Card 3    Similar Card 4  \\\n",
       "0        Ur-Golem's Eye        Ur-Golem's Eye      Thran Dynamo   \n",
       "1      Thrilling Encore        Urborg Justice        Fresh Meat   \n",
       "2  Vines of the Recluse       Shape the Sands   Crossbow Ambush   \n",
       "3      Mephitic Draught         Infernal Idol  Skeletal Scrying   \n",
       "4     Bloodrite Invoker    Kalastria Highborn    Sanctum Seeker   \n",
       "5        Ur-Golem's Eye        Ur-Golem's Eye      Thran Dynamo   \n",
       "6   Let the Galaxy Burn   Mordor on the March      Fiery Encore   \n",
       "7       Shape the Sands  Vines of the Recluse          Silk Net   \n",
       "8      Mephitic Draught         Infernal Idol   Profane Memento   \n",
       "9     Bloodrite Invoker    Kalastria Highborn   Shattered Angel   \n",
       "\n",
       "                         Similar Card 5  \\\n",
       "0                                Wastes   \n",
       "1                        Faith's Reward   \n",
       "2                              Silk Net   \n",
       "3                    Cut of the Profits   \n",
       "4                Herald of the Pantheon   \n",
       "5                      Bloodstone Cameo   \n",
       "6           All of History, All at Once   \n",
       "7                              Silk Net   \n",
       "8  Tithing Blade // Consuming Sepulcher   \n",
       "9                Herald of the Pantheon   \n",
       "\n",
       "                            Similar Card 6  \\\n",
       "0  Riverglide Pathway // Lavaglide Pathway   \n",
       "1                           Second Sunrise   \n",
       "2                       Gloomwidow's Feast   \n",
       "3                            Wooden Sphere   \n",
       "4                      Acolyte of Aclazotz   \n",
       "5                           Seashell Cameo   \n",
       "6                        Empty the Warrens   \n",
       "7                       Gloomwidow's Feast   \n",
       "8                          Thopter Foundry   \n",
       "9                             Deathgreeter   \n",
       "\n",
       "                            Similar Card 7  \\\n",
       "0  Riverglide Pathway // Lavaglide Pathway   \n",
       "1                      Let the Galaxy Burn   \n",
       "2                            Spidery Grasp   \n",
       "3                         Tablet of Epityr   \n",
       "4                      High Fae Negotiator   \n",
       "5                         Troll-Horn Cameo   \n",
       "6                          Urban Evolution   \n",
       "7                          Treetop Defense   \n",
       "8                          Druidic Satchel   \n",
       "9                          Bog-Strider Ash   \n",
       "\n",
       "                               Similar Card 8  \\\n",
       "0  Barkchannel Pathway // Tidechannel Pathway   \n",
       "1                                  Fatal Push   \n",
       "2                                    Aim High   \n",
       "3                              Urza's Chalice   \n",
       "4                           Judge of Currents   \n",
       "5                           Drake-Skull Cameo   \n",
       "6                         Escape to the Wilds   \n",
       "7                                    Aim High   \n",
       "8                         Ivory Crane Netsuke   \n",
       "9                      Guardian of Cloverdell   \n",
       "\n",
       "                               Similar Card 9       Similar Card 10  \n",
       "0  Barkchannel Pathway // Tidechannel Pathway         Great Furnace  \n",
       "1                             Cradle to Grave      Force of Despair  \n",
       "2                             Treetop Defense         Aerial Volley  \n",
       "3                                 Crystal Rod              Soul Net  \n",
       "4                            Inspiring Cleric        Centaur Healer  \n",
       "5                               Great Furnace                Wastes  \n",
       "6                               Flesh Allergy        Galvanic Relay  \n",
       "7                               Spidery Grasp     Angelic Ascension  \n",
       "8                                 Guild Globe    Sphere of the Suns  \n",
       "9                                Blood Seeker  Bleak Coven Vampires  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['Model Name', 'Card Name'] + [f'Similar Card {i+1}' for i in range(10)]\n",
    "df = pd.DataFrame(model_predictions, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = 'similar_cards.csv'\n",
    "df.to_csv(output_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters_degree_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
