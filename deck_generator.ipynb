{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 19:34:02.293014: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 19:34:02.864731: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTGDeckGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(MTGDeckGenerator, self).__init__()\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = layers.GRU(hidden_dim, return_state=True, return_sequences=True)\n",
    "        self.fc = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, commander_card):\n",
    "        embedded = self.embedding(commander_card)  \n",
    "        output, _ = self.gru(embedded)  \n",
    "        logits = self.fc(output)  \n",
    "        return logits\n",
    "\n",
    "    def generate_deck(self, commander_card, num_cards_to_generate):\n",
    "        embedded = self.embedding(commander_card)\n",
    "        hidden = tf.zeros((1, self.gru.units))\n",
    "        generated_deck = []\n",
    "        current_input = embedded\n",
    "\n",
    "        for _ in range(num_cards_to_generate):\n",
    "            output, hidden = self.gru(current_input, initial_state=hidden)\n",
    "            logits = self.fc(output[:, -1, :])\n",
    "            next_card = tf.argmax(logits, axis=-1)\n",
    "            generated_deck.append(next_card.numpy()[0])\n",
    "            current_input = self.embedding(tf.expand_dims(next_card, axis=0))\n",
    "\n",
    "        return generated_deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the dataset\n",
    "def tokenize_dataset(decks):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True,\n",
    "                             lower=True,\n",
    "                             filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "    \n",
    "    tokenizer.fit_on_texts(decks)\n",
    "    tokenized_decks = tokenizer.texts_to_sequences(decks)\n",
    "    return tokenized_decks, tokenizer\n",
    "\n",
    "# Training the model\n",
    "def train_model(model, tokenized_decks, epochs=10, batch_size=32):\n",
    "    input_cards = []\n",
    "    target_cards = []\n",
    "    \n",
    "    for deck in tokenized_decks:\n",
    "        input_cards.append(deck[:-1])\n",
    "        target_cards.append(deck[1:])\n",
    "    \n",
    "    input_cards = tf.keras.preprocessing.sequence.pad_sequences(input_cards, padding='post')\n",
    "    target_cards = tf.keras.preprocessing.sequence.pad_sequences(target_cards, padding='post')\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(input_cards, target_cards, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "def save_model(model, path=\"mtg_deck_generator_model.keras\"):\n",
    "    model.save(path)\n",
    "    print(f\"Model saved at {path}\")\n",
    "\n",
    "# Loading the model\n",
    "def load_model(path=\"mtg_deck_generator_model.keras\"):\n",
    "    model = tf.keras.models.load_model(path, custom_objects={\"MTGDeckGenerator\": MTGDeckGenerator})\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Dictionary: {'giadafontofhope': 'Giada, Font of Hope', 'blacklotus': 'Black Lotus', 'timewalk': 'Time Walk'}\n",
      "Formatted List: ['giadafontofhope', 'blacklotus', 'timewalk']\n",
      "Global Dictionary: {'giadafontofhope': 'Giada, Font of Hope', 'blacklotus': 'Black Lotus', 'timewalk': 'Time Walk', 'solring': 'Sol Ring'}\n",
      "Formatted List: ['giadafontofhope', 'solring', 'timewalk']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "card_dict_global = {}\n",
    "def process_global_card_data(card_names):\n",
    "    global card_dict_global\n",
    "    formatted_list = []\n",
    "    for name in card_names:\n",
    "        formatted_name = ''.join(char.lower() for char in name if char not in string.punctuation and not char.isspace())\n",
    "        if formatted_name not in card_dict_global:\n",
    "            card_dict_global[formatted_name] = name\n",
    "        formatted_list.append(formatted_name)\n",
    "    return formatted_list\n",
    "\n",
    "card_list = ['Giada, Font of Hope', 'Black Lotus', 'Time Walk']\n",
    "formatted_list = process_global_card_data(card_list)\n",
    "print(\"Global Dictionary:\", card_dict_global)\n",
    "print(\"Formatted List:\", formatted_list)\n",
    "\n",
    "card_list = ['Giada, Font of Hope', 'Sol Ring', 'Time Walk']\n",
    "formatted_list = process_global_card_data(card_list)\n",
    "print(\"Global Dictionary:\", card_dict_global)\n",
    "print(\"Formatted List:\", formatted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Giada, Font of Hope',\n",
       "  'Mesa Cavalier',\n",
       "  'Exorcise',\n",
       "  'Terramorphic Expanse',\n",
       "  'Angel of the Ruins',\n",
       "  'Plains',\n",
       "  'Palace Sentinels',\n",
       "  'Secluded Courtyard',\n",
       "  'Sol Ring',\n",
       "  'Fleeting Flight',\n",
       "  'Divine Resilience',\n",
       "  'Uncharted Haven',\n",
       "  'Dusk // Dawn',\n",
       "  'Hidden Grotto',\n",
       "  'Sinew Dancer',\n",
       "  'Plated Onslaught',\n",
       "  'Evolving Wilds',\n",
       "  'Banishing Light',\n",
       "  'Pearl Medallion',\n",
       "  'Renewed Faith',\n",
       "  'Feather of Flight',\n",
       "  'Vexing Bauble',\n",
       "  'Crackdown',\n",
       "  'Moonlit Wake',\n",
       "  'Charitable Levy',\n",
       "  'Dawn of a New Age',\n",
       "  'Forsake the Worldly',\n",
       "  \"Valkyrie's Call\",\n",
       "  'Mace of the Valiant',\n",
       "  'Dazzling Angel',\n",
       "  'Command Tower',\n",
       "  'Seized from Slumber',\n",
       "  'Joust Through',\n",
       "  'Vanguard Seraph',\n",
       "  'Take Up the Shield',\n",
       "  'Angel of Finality',\n",
       "  'Serra Angel',\n",
       "  'Decree of Justice',\n",
       "  'Angelic Destiny',\n",
       "  'Localized Destruction',\n",
       "  'Scale Blessing',\n",
       "  'Goldvein Pick',\n",
       "  \"Danitha, Benalia's Hope\",\n",
       "  'Unstable Obelisk',\n",
       "  'Dazzling Theater // Prop Room',\n",
       "  'Swiftfoot Boots',\n",
       "  'Lyra Dawnbringer',\n",
       "  'Crystal Barricade',\n",
       "  'Path to Exile',\n",
       "  'Restoration Angel',\n",
       "  \"Rogue's Passage\",\n",
       "  'Inspiring Overseer',\n",
       "  'Sublime Exhalation',\n",
       "  'Archway Angel',\n",
       "  'Day of Judgment',\n",
       "  'Split Up',\n",
       "  'Raise the Past',\n",
       "  'Selfless Spirit',\n",
       "  'Crib Swap',\n",
       "  'Loyal Unicorn',\n",
       "  'Celestial Armor',\n",
       "  'Angel of Vitality',\n",
       "  'Prayer of Binding',\n",
       "  'Youthful Valkyrie',\n",
       "  'Make Your Move',\n",
       "  'Exotic Orchard',\n",
       "  'Herald of Eternal Dawn',\n",
       "  'Dog Umbra',\n",
       "  'Repel the Vile',\n",
       "  'Banish from Edoras',\n",
       "  'Spectral Grasp',\n",
       "  'Exemplar of Light',\n",
       "  'Sword of the Animist',\n",
       "  'Lupinflower Village',\n",
       "  'Haystack',\n",
       "  \"Kirtar's Wrath\",\n",
       "  \"Maze's End\",\n",
       "  'Surgical Suite // Hospital Room',\n",
       "  'Impeccable Timing',\n",
       "  'Expel the Unworthy',\n",
       "  'Witch Enchanter'],\n",
       " ['giadafontofhope',\n",
       "  'mesacavalier',\n",
       "  'exorcise',\n",
       "  'terramorphicexpanse',\n",
       "  'angeloftheruins',\n",
       "  'plains',\n",
       "  'palacesentinels',\n",
       "  'secludedcourtyard',\n",
       "  'solring',\n",
       "  'fleetingflight',\n",
       "  'divineresilience',\n",
       "  'unchartedhaven',\n",
       "  'duskdawn',\n",
       "  'hiddengrotto',\n",
       "  'sinewdancer',\n",
       "  'platedonslaught',\n",
       "  'evolvingwilds',\n",
       "  'banishinglight',\n",
       "  'pearlmedallion',\n",
       "  'renewedfaith',\n",
       "  'featherofflight',\n",
       "  'vexingbauble',\n",
       "  'crackdown',\n",
       "  'moonlitwake',\n",
       "  'charitablelevy',\n",
       "  'dawnofanewage',\n",
       "  'forsaketheworldly',\n",
       "  'valkyriescall',\n",
       "  'maceofthevaliant',\n",
       "  'dazzlingangel',\n",
       "  'commandtower',\n",
       "  'seizedfromslumber',\n",
       "  'joustthrough',\n",
       "  'vanguardseraph',\n",
       "  'takeuptheshield',\n",
       "  'angeloffinality',\n",
       "  'serraangel',\n",
       "  'decreeofjustice',\n",
       "  'angelicdestiny',\n",
       "  'localizeddestruction',\n",
       "  'scaleblessing',\n",
       "  'goldveinpick',\n",
       "  'danithabenaliashope',\n",
       "  'unstableobelisk',\n",
       "  'dazzlingtheaterproproom',\n",
       "  'swiftfootboots',\n",
       "  'lyradawnbringer',\n",
       "  'crystalbarricade',\n",
       "  'pathtoexile',\n",
       "  'restorationangel',\n",
       "  'roguespassage',\n",
       "  'inspiringoverseer',\n",
       "  'sublimeexhalation',\n",
       "  'archwayangel',\n",
       "  'dayofjudgment',\n",
       "  'splitup',\n",
       "  'raisethepast',\n",
       "  'selflessspirit',\n",
       "  'cribswap',\n",
       "  'loyalunicorn',\n",
       "  'celestialarmor',\n",
       "  'angelofvitality',\n",
       "  'prayerofbinding',\n",
       "  'youthfulvalkyrie',\n",
       "  'makeyourmove',\n",
       "  'exoticorchard',\n",
       "  'heraldofeternaldawn',\n",
       "  'dogumbra',\n",
       "  'repelthevile',\n",
       "  'banishfromedoras',\n",
       "  'spectralgrasp',\n",
       "  'exemplaroflight',\n",
       "  'swordoftheanimist',\n",
       "  'lupinflowervillage',\n",
       "  'haystack',\n",
       "  'kirtarswrath',\n",
       "  'mazesend',\n",
       "  'surgicalsuitehospitalroom',\n",
       "  'impeccabletiming',\n",
       "  'expeltheunworthy',\n",
       "  'witchenchanter'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"data/mtg_decks.pkl\"\n",
    "\n",
    "card_dict_global = {}\n",
    "decks = load_data(path)\n",
    "better_decks = []\n",
    "for deck in decks:\n",
    "    better_decks.append(process_global_card_data(deck))\n",
    "\n",
    "\n",
    "decks[0], better_decks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_decks, tokenizer = tokenize_dataset(better_decks[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0014 - loss: 6.3452\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1  # Adjust based on dataset size\n",
    "print(vocab_size)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "\n",
    "\n",
    "\n",
    "model = MTGDeckGenerator(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "train_model(model, tokenized_decks, epochs=1, batch_size=1028)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: 65 (of type <class 'int'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m commander_card \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([tokenizer\u001b[38;5;241m.\u001b[39mword_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgiadafontofhope\u001b[39m\u001b[38;5;124m'\u001b[39m]])  \u001b[38;5;66;03m# Example token for \"Zur the Enchanter\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m num_cards_to_generate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m65\u001b[39m\n\u001b[0;32m---> 11\u001b[0m deck \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommander_card\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cards_to_generate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(deck)\n",
      "File \u001b[0;32m~/masters_degree/masters_degree_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/masters_degree/masters_degree_env/lib/python3.10/site-packages/keras/src/layers/layer.py:753\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mflatten(args):\n\u001b[1;32m    750\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, KerasTensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mis_tensor(\n\u001b[1;32m    751\u001b[0m             arg\n\u001b[1;32m    752\u001b[0m         ):\n\u001b[0;32m--> 753\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    754\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly input tensors may be passed as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    755\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional arguments. The following argument value \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    756\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould be passed as a keyword argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    757\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    758\u001b[0m             )\n\u001b[1;32m    760\u001b[0m \u001b[38;5;66;03m# Caches info about `call()` signature, args, kwargs.\u001b[39;00m\n\u001b[1;32m    761\u001b[0m call_spec \u001b[38;5;241m=\u001b[39m CallSpec(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_signature, args, kwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: 65 (of type <class 'int'>)"
     ]
    }
   ],
   "source": [
    "# save_model(model)\n",
    "# model = load_model()\n",
    "\n",
    "\n",
    "commander_card = tf.constant([tokenizer.word_index['giadafontofhope']])\n",
    "num_cards_to_generate = 65\n",
    "\n",
    "deck = model(commander_card, num_cards_to_generate)\n",
    "print(deck)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters_degree_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
